<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>RNN lab :: Guillaume Grandineau Blog</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="The main goal of this lab is to implement a model that is able to caption an image with a text that describes this image.
There will be 5 parts for this lab: Dataset description and pre-processing
 Encoder: pre-trained CNN Decoder: LSTM and Encoder-Decoder model Training Interference  Firstly, we describe our dataset: The one we use is COCO which is famous for object detection and image captioning. We will use here the 2014 version since it has 83k images for training, 41k for validation and 41k for test." />
<meta name="keywords" content=", " />
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://guillaumeg9153.github.io/MyBlog/posts/labrnn/" />




<link rel="stylesheet" href="https://guillaumeg9153.github.io/MyBlog/assets/style.css">

  <link rel="stylesheet" href="https://guillaumeg9153.github.io/MyBlog/assets/red.css">






<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://guillaumeg9153.github.io/MyBlog/img/apple-touch-icon-144-precomposed.png">

  <link rel="shortcut icon" href="https://guillaumeg9153.github.io/MyBlog/img/favicon/red.png">



<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="RNN lab :: Guillaume Grandineau Blog">
<meta property="og:description" content="The main goal of this lab is to implement a model that is able to caption an image with a text that describes this image.
There will be 5 parts for this lab: Dataset description and pre-processing
 Encoder: pre-trained CNN Decoder: LSTM and Encoder-Decoder model Training Interference  Firstly, we describe our dataset: The one we use is COCO which is famous for object detection and image captioning. We will use here the 2014 version since it has 83k images for training, 41k for validation and 41k for test." />
<meta property="og:url" content="https://guillaumeg9153.github.io/MyBlog/posts/labrnn/" />
<meta property="og:site_name" content="RNN lab" />

  <meta property="og:image" content="https://guillaumeg9153.github.io/MyBlog">

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">













</head>
<body class="red">


<div class="container center headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="https://guillaumeg9153.github.io/MyBlog">
  <div class="logo">
    Guillaume Grandineau blog
  </div>
</a>

    </div>
    <div class="menu-trigger">menu</div>
  </div>
  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="https://guillaumeg9153.github.io/MyBlog/posts/labrnn/">RNN lab</a></h1>
  <div class="post-meta">
    
    
  </div>

  
  <span class="post-tags">
    
    #<a href="https://guillaumeg9153.github.io/MyBlog/tags/"></a>&nbsp;
    
    #<a href="https://guillaumeg9153.github.io/MyBlog/tags/"></a>&nbsp;
    
  </span>
  

  

  

  <div class="post-content"><div>
        <p>The main goal of this lab is to implement a model that is able to caption an image with a text that describes this image.</p>
<p>There will be 5 parts for this lab: Dataset description and pre-processing</p>
<pre><code>              Encoder: pre-trained CNN
			  
              Decoder: LSTM and Encoder-Decoder model
			  
              Training
			  
              Interference
</code></pre>
<p>Firstly, we describe our dataset: The one we use is COCO which is famous for object detection and image captioning. We will use here the 2014 version since it has 83k images for training, 41k for validation and 41k for test. In this laboratory we will only use 10k, 2k and 2k for respectively training, validation and testing.</p>
<p>For each set the data is separated in an image and a caption folder. Images are in JPG while captions are stored in a json file, this file specifies to which caption an image is linked.</p>
<p>The first step is to access this dataset:</p>
<p><img src="/MyBlog/hugo1.jpg" alt="image info"></p>
<p>This is one of the images we displayed:</p>
<p><img src="/MyBlog/hugo2.jpg" alt="image info"></p>
<p>Then we linked images and captions:</p>
<p><img src="/MyBlog/hugo3.jpg" alt="image info"></p>
<p>Thanks to this function we can load a dictionary.</p>
<p>Here is an example of captions linked to an image:</p>
<p><img src="/MyBlog/hugo4.jpg" alt="image info"></p>
<p>Then we create function that permit to retrieve words from these captions.</p>
<p><img src="/MyBlog/hugo5.jpg" alt="image info"></p>
<p>Then we compute a function that permit to retrieve the unique words.</p>
<p><img src="/MyBlog/hugo6.jpg" alt="image info"></p>
<p>This permit to get the vocabulary size. So we are able to turn words into vectors.</p>
<p><img src="/MyBlog/hugo7.jpg" alt="image info"></p>
<p><img src="/MyBlog/hugo8.jpg" alt="image info"></p>
<p>We also have to do the inverse operation:</p>
<p><img src="/MyBlog/hugo9.jpg" alt="image info"></p>
<h1 id="encoder">Encoder<a href="#encoder" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<p>In this part we will implement our model that has to be composed of an encoder and a decoder.
The encoder processes the input sequence and extracts features from it. These features will be decoded to produce the output sequence.</p>
<p>Our encoder is a pre-trained CNN on ImageNet.</p>
<p>After we need to encode images:</p>
<p><img src="/MyBlog/hugo10.jpg" alt="image info"></p>
<p>And then save these images from the encoder and load them from a pickle file.</p>
<p><img src="/MyBlog/hugo11.jpg" alt="image info"></p>
<h1 id="decoder">Decoder<a href="#decoder" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<p>We will use a LSTM as the decoder, it takes inputs = [images Features, captions] and outputs = next predicted words.</p>
<p><img src="/MyBlog/hugo12.jpg" alt="image info"></p>

      </div></div>

  
  
<div class="pagination">
    <div class="pagination__title">
        <span class="pagination__title-h">Read other posts</span>
        <hr />
    </div>
    <div class="pagination__buttons">
        
        <span class="button previous">
            <a href="https://guillaumeg9153.github.io/MyBlog/posts/lecture4/">
                <span class="button__icon">←</span>
                <span class="button__text">Lecture 4</span>
            </a>
        </span>
        
        
        <span class="button next">
            <a href="https://guillaumeg9153.github.io/MyBlog/posts/rnnlecture/">
                <span class="button__text">RNN Lecture</span>
                <span class="button__icon">→</span>
            </a>
        </span>
        
    </div>
</div>

  

  

</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2020 Powered by <a href="http://gohugo.io">Hugo</a></span>
    
        <span>:: Theme made by <a href="https://twitter.com/panr">panr</a></span>
      </div>
  </div>
</footer>

<script src="https://guillaumeg9153.github.io/MyBlog/assets/main.js"></script>
<script src="https://guillaumeg9153.github.io/MyBlog/assets/prism.js"></script>





  
</div>

</body>
</html>
